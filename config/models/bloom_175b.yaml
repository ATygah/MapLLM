model_name: "bloom-176b"
attention_type: "self"
vocab_size: 250880
embed_dim: 14336
max_seq_len: 2048
batch_size: 1
seq_len: 2048
kv_seq_len: 2048
dtype: "float16"
num_heads: 112
num_layers: 1
mlp_expansion_factor: 4
mlp_activation: "gelu"
norm_type: "prelayernorm"
dropout_rate: 0.0
architecture: "gpt2"
positional_encoding: "rotary"
